{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "e2c0cf9f-026a-43f4-b5d3-8048f40a7a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\ylazz\\anaconda3\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a96ec31-9276-469a-8cf5-f027ba3e0e62",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This project demonstrates the implementation of a text classifier using Markov Models to distinguish between the writing styles of Edgar Allan Poe and Robert Frost. Rather than using pre-built ML libraries, the classifier is built from scratch using custom transition matrices, demonstrating deep understanding of probability theory and algorithm implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e461f7-7843-4a10-8f3c-d11de90f5112",
   "metadata": {},
   "source": [
    "### Data Loading and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e490c8e9-4875-4b54-8f85-7734e07b2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c879df29-fe20-4f47-8fc7-39606592ef06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edgar_allan_poe.txt already exists. Skipping download.\n",
      "robert_frost.txt already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wget\n",
    "import os\n",
    "\n",
    "urls = [\n",
    "    'https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt',\n",
    "    'https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt'\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    # Extract the filename from the URL\n",
    "    filename = url.split('/')[-1]\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(filename):\n",
    "        downloaded_file = wget.download(url)\n",
    "        print(f\"\\nDownloaded: {downloaded_file}\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4bda2c-7304-48a0-9e93-786b46bf90e0",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ce37dd81-46f0-47a4-9c6a-50fbe6a6dbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LO! Death hath rear'd himself a throne\n",
      "In a strange city, all alone,\n",
      "Far down within the dim west\n",
      "Where the good, and the bad, and the worst, and the best,\n",
      "Have gone to their eternal rest.\n",
      "\n",
      "There shrines, and palaces, and towers\n",
      "Are not like any thing of ours\n",
      "Oh no! O no! ours never loom\n",
      "To heaven with that ungodly gloom!\n"
     ]
    }
   ],
   "source": [
    "with open(\"edgar_allan_poe.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for _ in range(10):\n",
    "        print(file.readline().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0d09fc98-c6c9-4fb1-8947-669e68c156aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two roads diverged in a yellow wood,\n",
      "And sorry I could not travel both\n",
      "And be one traveler, long I stood\n",
      "And looked down one as far as I could\n",
      "To where it bent in the undergrowth;\n",
      "\n",
      "Then took the other, as just as fair,\n",
      "And having perhaps the better claim\n",
      "Because it was grassy and wanted wear,\n",
      "Though as for that the passing there\n"
     ]
    }
   ],
   "source": [
    "with open(\"robert_frost.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for _ in range (10):\n",
    "        print(file.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6b174-e50a-495f-832f-1dc1337bb782",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "72302d0b-69ac-462a-8f12-9b3ea1f35d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the first list that will contain each sentence, separated by a comma, of each set of poems\n",
    "\n",
    "authors_list = []\n",
    "labels = []\n",
    "\n",
    "with open(\"edgar_allan_poe.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        if line.strip(): #removes any leading/trailing spaces (or newline characters) from the line and checks if thereâ€™s anything left. \n",
    "                         # If the line is empty, it skips it. Basically, if the line, after stripping, is not empty, append that line\n",
    "                            # if it's empty, skip\n",
    "            authors_list.append(line.strip().translate(str.maketrans('', '', string.punctuation)))\n",
    "            labels.append(0)\n",
    "\n",
    "with open(\"robert_frost.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        if line.strip():\n",
    "            authors_list.append(line.strip().translate(str.maketrans('', '', string.punctuation)))\n",
    "            labels.append(1)\n",
    "\n",
    "#authors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a228035b-f6ee-4077-8ca5-ff3727a8a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "18d4d2dd-1f02-4074-b219-e071e7d7aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(authors_list, labels, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "351f69e7-69e2-4f4d-9b3d-e593031e15d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Does the rain seem to you to cool the eyes',\n",
       " 'And perhaps she will come still unafraid',\n",
       " 'These cheeks where the worm never dies',\n",
       " 'The ledges show lines ruled southeastnorthwest',\n",
       " 'If I could see it or else mow the room']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4af51a5b-fb0c-41f9-8772-82e24c3ab955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1615, 539)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feedd900-7632-432e-91dc-c06ba2befeeb",
   "metadata": {},
   "source": [
    "### Building the Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8591d419-4736-493f-9a2e-194680c857f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize words while creating a dictionary of word --> index\n",
    "# this is a dictionary of unique words, as each word gets 1 index value. Later on, when we convert sentences to indices, the \n",
    "# word \"the\" for example will always get the same index value as written in word_to_idx\n",
    "idx = 1\n",
    "word_to_idx = {'<UNK>':0}\n",
    "\n",
    "for line in X_train:\n",
    "    words = word_tokenize(line.lower()) #this tokenizes each word\n",
    "    for word in words:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = idx\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "de2d502c-6ab8-454e-a3b4-01f01942286d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<UNK>', 0),\n",
       " ('does', 1),\n",
       " ('the', 2),\n",
       " ('rain', 3),\n",
       " ('seem', 4),\n",
       " ('to', 5),\n",
       " ('you', 6),\n",
       " ('cool', 7),\n",
       " ('eyes', 8),\n",
       " ('and', 9)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_idx.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "420a7609-0c88-4748-bcc0-774feda08af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_split = []\n",
    "for sentence in X_train:\n",
    "    words = sentence.split()\n",
    "    X_train_split.append(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "67391940-0364-4747-960b-299224216892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Does', 'the', 'rain', 'seem', 'to', 'you', 'to', 'cool', 'the', 'eyes'],\n",
       " ['And', 'perhaps', 'she', 'will', 'come', 'still', 'unafraid'],\n",
       " ['These', 'cheeks', 'where', 'the', 'worm', 'never', 'dies'],\n",
       " ['The', 'ledges', 'show', 'lines', 'ruled', 'southeastnorthwest'],\n",
       " ['If', 'I', 'could', 'see', 'it', 'or', 'else', 'mow', 'the', 'room'],\n",
       " ['Winds', 'blow', 'the', 'open', 'grassy', 'places', 'bleak'],\n",
       " ['Tis', 'said', 'that', 'when'],\n",
       " ['Such', 'as', 'it', 'is', 'It', 'isnt', 'worth', 'the', 'mortgage'],\n",
       " ['Will', 'start', 'which', 'lately', 'slept', 'in', 'apathy'],\n",
       " ['Which', 'is', 'wrong']]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_split[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "823e3d23-9b8d-4ad9-b633-d86631af244f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 5, 7, 2, 8],\n",
       " [9, 10, 11, 12, 13, 14, 15],\n",
       " [16, 17, 18, 2, 19, 20, 21],\n",
       " [2, 22, 23, 24, 25, 26],\n",
       " [27, 28, 29, 30, 31, 32, 33, 34, 2, 35],\n",
       " [36, 37, 2, 38, 39, 40, 41],\n",
       " [42, 43, 44, 45],\n",
       " [46, 47, 31, 48, 31, 49, 50, 2, 51],\n",
       " [12, 52, 53, 54, 55, 56, 57],\n",
       " [53, 48, 58]]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_idx = []\n",
    "for sentence in X_train_split:\n",
    "    sentence_indices = []\n",
    "    for word in sentence:\n",
    "        index = word_to_idx.get(word.lower(),0) #dictionary method: looks up the word (lowered) in the dictionary and return the index if found\n",
    "                                                # if it's not found, it returns a 0\n",
    "        sentence_indices.append(index) #Adds the index number to the sentence_indices list\n",
    "    X_train_idx.append(sentence_indices) # After converting all words in a sentence to numbers, adds that list of numbers to our main list\n",
    "\n",
    "X_train_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7f2ac088-b38f-44f8-aa2c-01f818425947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[303, 18, 173, 262, 123, 69, 0, 0],\n",
       " [9, 0, 147],\n",
       " [0, 47, 104, 1396, 5, 0],\n",
       " [67, 272, 56, 748, 768, 467, 177, 9, 554],\n",
       " [5, 2, 180, 18, 147, 151, 282, 0, 788],\n",
       " [1872, 1775, 173, 2156, 56, 2, 858],\n",
       " [56, 3, 0, 467, 203, 27, 31, 0],\n",
       " [0, 226, 6, 289, 290, 62, 0],\n",
       " [237, 1471, 223, 265, 78, 561, 2, 2316, 67],\n",
       " [9, 116, 28, 29, 59, 0, 1168]]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do the same for the testing data\n",
    "\n",
    "X_test_split = []\n",
    "for sentence in X_test:\n",
    "    words = sentence.split()\n",
    "    X_test_split.append(words)\n",
    "\n",
    "\n",
    "X_test_idx = []\n",
    "for sentence in X_test_split:\n",
    "    sentence_indices = []\n",
    "    for word in sentence:\n",
    "        index = word_to_idx.get(word.lower(),0) #dictionary method: looks up the word (lowered) in the dictionary and return the index if found\n",
    "                                                # if it's not found, it returns a 0\n",
    "        sentence_indices.append(index) #Adds the index number to the sentence_indices list\n",
    "    X_test_idx.append(sentence_indices) # After converting all words in a sentence to numbers, adds that list of numbers to our main list\n",
    "\n",
    "X_test_idx[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "11b750c4-0dbb-497a-bfcf-67ca3bfa39fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the training data by author so as to have 2 markov models\n",
    "poe_sentences = []\n",
    "frost_sentences = []\n",
    "\n",
    "for index, author in enumerate(y_train):\n",
    "    for index2, sentence in enumerate(X_train_idx):\n",
    "        if index == index2 and author == 0:\n",
    "            poe_sentences.append(sentence)\n",
    "        elif index == index2 and author == 1:\n",
    "            frost_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "88d06c60-7fd4-438a-a0dd-b73b7e4df604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[16, 17, 18, 2, 19, 20, 21],\n",
       " [42, 43, 44, 45],\n",
       " [12, 52, 53, 54, 55, 56, 57],\n",
       " [56, 2, 65, 66, 67, 68],\n",
       " [2, 75, 76, 77]]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poe_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f45e1f5e-45a0-4045-9cf5-f646ff7e077e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 5, 7, 2, 8],\n",
       " [9, 10, 11, 12, 13, 14, 15],\n",
       " [2, 22, 23, 24, 25, 26],\n",
       " [27, 28, 29, 30, 31, 32, 33, 34, 2, 35],\n",
       " [36, 37, 2, 38, 39, 40, 41]]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frost_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050991f-a708-4e05-89ac-41f13614b623",
   "metadata": {},
   "source": [
    "### Transition Matrix Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "db519270-4b62-4ff1-9641-c7ec1ddefa4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poe_transition_matrix = np.zeros((len(word_to_idx), len(word_to_idx)))\n",
    "poe_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8018b186-8616-4a99-b537-fafa80ede7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00039857, 0.00039857, 0.00079713, ..., 0.00039857, 0.00039857,\n",
       "        0.00039857],\n",
       "       [0.00039936, 0.00039936, 0.00039936, ..., 0.00039936, 0.00039936,\n",
       "        0.00039936],\n",
       "       [0.00036778, 0.00036778, 0.00036778, ..., 0.00036778, 0.00036778,\n",
       "        0.00036778],\n",
       "       ...,\n",
       "       [0.00039936, 0.00039936, 0.00039936, ..., 0.00039936, 0.00039936,\n",
       "        0.00039936],\n",
       "       [0.00039936, 0.00039936, 0.00039936, ..., 0.00039936, 0.00039936,\n",
       "        0.00039936],\n",
       "       [0.00039936, 0.00039936, 0.00039936, ..., 0.00039936, 0.00039936,\n",
       "        0.00039936]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sentence in poe_sentences:\n",
    "    for i in range(len(sentence)-1):\n",
    "        current_word = sentence[i]\n",
    "        next_word = sentence[i+1]\n",
    "        poe_transition_matrix[current_word][next_word] +=1\n",
    "\n",
    "# Add-one smoothing (add 1 to all counts)\n",
    "poe_transition_matrix += 1\n",
    "\n",
    "#Convert counts to probabilities\n",
    "row_sum = poe_transition_matrix.sum(axis = 1)\n",
    "poe_transition_matrix = poe_transition_matrix/row_sum[:,np.newaxis] #NumPy [:,np.newaxis] will broadcast the single column across all columns in the matrix. \n",
    "                                                            #Conceptually, it \"duplicates\" the column as many times as there are columns in the matrix:\n",
    "\n",
    "poe_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5319b40e-3160-4a3f-aafe-4865b255e20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2509., 2504., 2719., ..., 2504., 2504., 2504.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sum #that's why we use [:,np.newaxis], to change this 1D array (n,) into a column vector (n,1). This way the array \n",
    "#Becomes a vector of row n and column 1. Then we broadcast that one column as many times as there are columns in the \n",
    "#transition matrix, so as to divide every number in the first row of the matrix by the first number of the \n",
    "#column vector (in this case 2519), then every nuber in the second row of the matrix by the second number in the column vector (2513), etc etc. These\n",
    "# numbers are obviously the sum of each row of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "158a160b-7f7b-4af2-a462-c98174751469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.82763955, -7.82763955, -7.13449237, ..., -7.82763955,\n",
       "        -7.82763955, -7.82763955],\n",
       "       [-7.82564473, -7.82564473, -7.82564473, ..., -7.82564473,\n",
       "        -7.82564473, -7.82564473],\n",
       "       [-7.90801944, -7.90801944, -7.90801944, ..., -7.90801944,\n",
       "        -7.90801944, -7.90801944],\n",
       "       ...,\n",
       "       [-7.82564473, -7.82564473, -7.82564473, ..., -7.82564473,\n",
       "        -7.82564473, -7.82564473],\n",
       "       [-7.82564473, -7.82564473, -7.82564473, ..., -7.82564473,\n",
       "        -7.82564473, -7.82564473],\n",
       "       [-7.82564473, -7.82564473, -7.82564473, ..., -7.82564473,\n",
       "        -7.82564473, -7.82564473]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since the numbers i the matrix are rather small, we take the log10\n",
    "poe_transition_matrix = np.log(poe_transition_matrix)\n",
    "poe_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5ca01ed1-7cc9-4aea-8774-862b38f323f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.82564473, -7.82564473, -7.82564473, ..., -7.82564473,\n",
       "        -7.82564473, -7.82564473],\n",
       "       [-7.8272409 , -7.8272409 , -7.13409372, ..., -7.8272409 ,\n",
       "        -7.8272409 , -7.8272409 ],\n",
       "       [-7.97212113, -7.97212113, -7.97212113, ..., -7.97212113,\n",
       "        -7.97212113, -7.97212113],\n",
       "       ...,\n",
       "       [-7.82604401, -7.82604401, -7.82604401, ..., -7.82604401,\n",
       "        -7.13289683, -7.82604401],\n",
       "       [-7.82564473, -7.82564473, -7.82564473, ..., -7.82564473,\n",
       "        -7.82564473, -7.82564473],\n",
       "       [-7.82604401, -7.82604401, -7.82604401, ..., -7.82604401,\n",
       "        -7.82604401, -7.82604401]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now do the same for frost\n",
    "frost_transition_matrix = np.zeros((len(word_to_idx), len(word_to_idx)))\n",
    "for sentence in frost_sentences:\n",
    "    for i in range(len(sentence)-1):\n",
    "        current_word = sentence[i]\n",
    "        next_word = sentence[i+1]\n",
    "        frost_transition_matrix[current_word][next_word] +=1\n",
    "\n",
    "# Add-one smoothing (add 1 to all counts)\n",
    "frost_transition_matrix += 1\n",
    "\n",
    "#Convert counts to probabilities\n",
    "row_sum = frost_transition_matrix.sum(axis = 1)\n",
    "frost_transition_matrix = frost_transition_matrix/row_sum[:,np.newaxis]\n",
    "frost_transition_matrix = np.log(frost_transition_matrix)\n",
    "frost_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc212f-f2d2-41b5-9fcb-7b43bb92a30a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6b3181d0-0a38-4fd1-9be5-c1b6dabe95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to calculate the priors (p(class = k)), meaning The prior probabilities p(class = k) \n",
    "#represent how likely each class (Poe or Frost) is before looking at any specific text. In your case, you need to calculate:\n",
    "\n",
    "#p(class = Poe)\n",
    "#p(class = Frost)\n",
    "\n",
    "#Basically how many training samples we have of each class divided by sum of all traning samples\n",
    "#Remember, Poe is 0 and Frost is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "37dc93c2-3345-4492-aa33-05b7a58ce3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32941176470588235"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_poe =  y_train.count(0)/(y_train.count(0)+y_train.count(1))\n",
    "probability_poe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "109c6ef0-cae8-4a6f-80ba-44eb48eda6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6705882352941176"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_frost = y_train.count(1)/(y_train.count(0)+y_train.count(1))\n",
    "probability_frost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e585ad54-e1d6-4cd6-b746-801c99802174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we are working with log, we convert these as well\n",
    "log_probability_poe = np.log(probability_poe)\n",
    "log_probability_frost = np.log(probability_frost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0893ee18-f5e9-4aca-9711-e680e8ee6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the prediction\n",
    "# The functon will take \n",
    "#1) an input (a single sentence which will be in the list of indices, like [1,4,7,9....]\n",
    "#2) Poe's transition matrix\n",
    "#3) Frost's transition matri\n",
    "#4) Log probabilities we calculated for Poe and Frost\n",
    "\n",
    "#For a sentence like [1, 4, 2, 7], we need to:\n",
    "\n",
    "#Look at each pair of consecutive words: (1,4), (4,2), (2,7)\n",
    "#For each pair, look up its probability in the transition matrix\n",
    "#Sum these log probabilities (remember, we use sum because we're working with logs, instead of multiplication)\n",
    "#Add the log prior probability at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c7abb-fac1-47ac-b5f6-26422600acf9",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7cb2e0a7-2282-43c2-9c32-3f47b297b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_author(sentence, poe_transition, frost_transition, log_prob_poe, log_prob_frost):\n",
    "\n",
    "    #Initial score\n",
    "    poe_score = log_prob_poe\n",
    "    frost_score = log_prob_frost \n",
    "\n",
    "    #Look at each consecutive pairs\n",
    "    for i in range(len(sentence)-1):\n",
    "        current_word = sentence[i]\n",
    "        next_word = sentence[i+1]\n",
    "\n",
    "        #Add transitional probabilitites\n",
    "        poe_score = poe_score + poe_transition[current_word][next_word]\n",
    "        frost_score = frost_score + frost_transition[current_word][next_word]\n",
    "\n",
    "    #return predicted author\n",
    "\n",
    "    return 0 if poe_score > frost_score else 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "78399a40-2e12-41c5-b5a2-b9b613e2ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = []\n",
    "for sentence in X_train_idx:\n",
    "    pred = predict_author(sentence, poe_transition_matrix, frost_transition_matrix, log_probability_poe, log_probability_frost)\n",
    "    train_predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d30ec1af-19e3-4dbd-8f63-25f7b295e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "550b14fd-caee-46f7-b4be-8dcbf6e4cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9975232198142415\n"
     ]
    }
   ],
   "source": [
    "# See accuracy on training data\n",
    "train_accuracy = (np.array(train_predictions) == y_train).sum() / len(y_train) #compares each element in train_predictions \n",
    "                                                                               #with the corresponding element in y_train. \n",
    "                                                                               #This produces a boolean array, where each element \n",
    "                                                                    #is True if the corresponding values in the two arrays match, and False otherwise\n",
    "print(f\"Training accuracy: {train_accuracy}\")\n",
    "\n",
    "#When you use .sum() on this boolean array, Python treats True as 1 and False as 0. \n",
    "#So, summing this array gives you the total count of matches between train_predictions and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "06aa15dc-9b2e-425d-8452-1698640a262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for test data\n",
    "test_predictions = []\n",
    "for sentence in X_test_idx:  # You'll need to create X_test_idx first!\n",
    "    pred = predict_author(sentence, poe_transition_matrix, frost_transition_matrix, \n",
    "                         log_probability_poe, log_probability_frost)\n",
    "    test_predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "783c36f0-d153-4eb7-b9a3-4d9e3cb9c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.8237476808905381\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = (np.array(test_predictions) == y_test).sum() / len(y_test)\n",
    "print(f\"Testing accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4158b9b9-2ca8-428b-9b98-2273d4f46ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "38e982dc-cd6b-4b8b-8c48-de910e621d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[104,  82],\n",
       "       [ 13, 340]], dtype=int64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_test = confusion_matrix(y_test, test_predictions)\n",
    "cm_test\n",
    "\n",
    "#    Predicted\n",
    "#    0 1\n",
    "# 0\n",
    "# 1\n",
    "#Actual\n",
    "\n",
    "#92 poems correctly identified as Poe that were actually Poe\n",
    "#92 poems identified as Frost tha were instead Poe\n",
    "#20 poems identified as Poe that were actually frost \n",
    "# 335 poems identified as Frost that were actually Frost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "eaf9ace5-0b5f-4090-a684-d848283d3aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 528,    4],\n",
       "       [   0, 1083]], dtype=int64)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_train = confusion_matrix(y_train, train_predictions)\n",
    "cm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "68d9bb25-0b3f-45c6-af81-a5777d00b4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8774193548387097"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_test = f1_score(y_test,test_predictions)\n",
    "f1_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "acee6fa2-bd61-4cb9-a806-c5c2a63af7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9981566820276497"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_train = f1_score(y_train,train_predictions)\n",
    "f1_score_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4765893c-7cad-4c3b-bdc9-f10e1a56c909",
   "metadata": {},
   "source": [
    "### Model Optimization Through Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "16d9fe96-b6b0-414e-b5ae-c20be6cfc4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(532, 1083)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets oversample Poe and undersample Frost\n",
    "\n",
    "len(poe_sentences), len(frost_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "df6e92f9-b273-4ed0-8ce4-f93fd236ee03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 744,  197,  671,  184,  982,  659,  602,  408, 1064,  202])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First get random indices\n",
    "random_indices = np.random.choice(len(frost_sentences), size=len(poe_sentences), replace=False)\n",
    "random_indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5175f015-b2cb-434b-9a86-0963522dc6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[884, 20, 12, 177, 5, 237, 598, 67, 2028],\n",
       " [850, 395, 18, 173, 851, 67, 399, 47, 852],\n",
       " [32, 955, 236, 67, 516, 222],\n",
       " [5, 211, 394, 272, 5, 461, 5, 812, 69, 813],\n",
       " [90, 47, 5, 174, 31, 348, 62, 2386]]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then use these indices to sample from frost_sentences\n",
    "sampled_frost = []\n",
    "for i in random_indices:\n",
    "    sampled_frost.append(frost_sentences[i])\n",
    "\n",
    "sampled_frost[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6ceb45bc-9ea6-4f35-af01-fa464fd11cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.82564473, -7.82564473, -7.82564473, ..., -7.82564473,\n",
       "        -7.82564473, -7.82564473],\n",
       "       [-7.82644314, -7.82644314, -7.82644314, ..., -7.82644314,\n",
       "        -7.82644314, -7.82644314],\n",
       "       [-7.89915348, -7.89915348, -7.89915348, ..., -7.89915348,\n",
       "        -7.89915348, -7.89915348],\n",
       "       ...,\n",
       "       [-7.82564473, -7.82564473, -7.82564473, ..., -7.82564473,\n",
       "        -7.82564473, -7.82564473],\n",
       "       [-7.82564473, -7.82564473, -7.82564473, ..., -7.82564473,\n",
       "        -7.82564473, -7.82564473],\n",
       "       [-7.82604401, -7.82604401, -7.82604401, ..., -7.82604401,\n",
       "        -7.82604401, -7.82604401]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now do the same for frost\n",
    "frost_transition_matrix2 = np.zeros((len(word_to_idx), len(word_to_idx)))\n",
    "for sentence in sampled_frost:\n",
    "    for i in range(len(sentence)-1):\n",
    "        current_word = sentence[i]\n",
    "        next_word = sentence[i+1]\n",
    "        frost_transition_matrix2[current_word][next_word] +=1\n",
    "\n",
    "# Add-one smoothing (add 1 to all counts)\n",
    "frost_transition_matrix2 += 1\n",
    "\n",
    "#Convert counts to probabilities\n",
    "row_sum = frost_transition_matrix2.sum(axis = 1)\n",
    "frost_transition_matrix2 = frost_transition_matrix2/row_sum[:,np.newaxis]\n",
    "frost_transition_matrix2 = np.log(frost_transition_matrix2)\n",
    "frost_transition_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "48537acb-a01c-4bb6-abfd-4ef4e7c24682",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = poe_sentences + sampled_frost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "effefa2b-f6df-416a-aa0d-6edbfa4ee345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1064"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "003cee6f-5715-4bfa-9920-41e3f26d4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = [0]*544 + [1]*544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "843f320d-20e6-41a6-b57f-4b5b75d669cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.7012987012987013\n"
     ]
    }
   ],
   "source": [
    "log_probability_poe = 0.5\n",
    "log_probability_frost = 0.5\n",
    "\n",
    "test_predictions = []\n",
    "for sentence in X_test_idx: \n",
    "    pred = predict_author(sentence, poe_transition_matrix, frost_transition_matrix2, \n",
    "                         log_probability_poe, log_probability_frost)\n",
    "    test_predictions.append(pred)\n",
    "\n",
    "test_accuracy = (np.array(test_predictions) == y_test).sum() / len(y_test)\n",
    "print(f\"Testing accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c64493-cf87-42ff-9c53-62c613669497",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "The model achieves 82% accuracy on the test set, demonstrating the effectiveness of custom-built Markov Models for text classification. Key achievements:\n",
    "\n",
    "Successful implementation of transition matrices from scratch\n",
    "\n",
    "Effective use of add-one smoothing for probability estimation\n",
    "\n",
    "Improved performance through data balancing\n",
    "\n",
    "Strong results without relying on pre-built ML libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f62767-42db-4f64-8963-ba760baead80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
